{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoblieNetV1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBJh8425aapI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtrBnjYwcNwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MobileNet_V1(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MobileNet_V1, self).__init__()\n",
        "\n",
        "        # 传统卷积层\n",
        "        def conv_bn(input, output, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input, output, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(output),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        \n",
        "        # depthwise卷积层\n",
        "        def conv_dw(input, output, stride):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input, input, 3, stride, 1, groups=input, bias=False),\n",
        "                nn.BatchNorm2d(input),\n",
        "                nn.ReLU(),\n",
        "                \n",
        "                nn.Conv2d(input, output, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(output),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            conv_bn(3, 32, 2),\n",
        "            conv_dw(32, 64, 1),\n",
        "            conv_dw(64, 128, 2),\n",
        "            conv_dw(128, 128, 1),\n",
        "            conv_dw(128, 256, 2),\n",
        "            conv_dw(256, 512, 2),\n",
        "            conv_dw(512, 512, 1),\n",
        "            conv_dw(512, 512, 1),\n",
        "            conv_dw(512, 512, 1),\n",
        "            conv_dw(512, 512, 1),\n",
        "            conv_dw(512, 512, 1),\n",
        "            conv_dw(512, 1024, 2),\n",
        "            conv_dw(1024, 1024, 1),\n",
        "            # nn.AvgPool2d(7),\n",
        "        )\n",
        "        self.fc = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1, 1024)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKKvAdg8aoJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    # transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpwBU0Vga2Rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "72afe995-1bf5-4022-b059-b4178eea87d1"
      },
      "source": [
        "train_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                          download=True, transform=transform)\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                          download=True, transform=transform)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYtrcXpvbImW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b33388c-431d-427f-bc1d-df6a26dee58d"
      },
      "source": [
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
        "\n",
        "x, y = iter(train_loader).next()\n",
        "print(\"x:\", x.shape, \"y:\", y.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: torch.Size([16, 3, 32, 32]) y: torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9MEvkd9b89s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 类别关键字\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbVFGExcCZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 模型\n",
        "model = MobileNet_V1().to(device)\n",
        "# print(model)\n",
        "# print(x.shape, \"x\")\n",
        "# print(model(x).shape, \"o\")\n",
        "# 损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#优化器\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWgHcNL0fjzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4cf1c72-7e81-4a6e-f5de-a34e60c6be3d"
      },
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    running_loss = 0\n",
        "    \n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "     \n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            print('[%d %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 1000))\n",
        "            running_loss = 0\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1  1000] loss: 2.031\n",
            "[1  2000] loss: 1.786\n",
            "[1  3000] loss: 1.666\n",
            "[2  1000] loss: 1.579\n",
            "[2  2000] loss: 1.523\n",
            "[2  3000] loss: 1.446\n",
            "[3  1000] loss: 1.355\n",
            "[3  2000] loss: 1.294\n",
            "[3  3000] loss: 1.237\n",
            "[4  1000] loss: 1.145\n",
            "[4  2000] loss: 1.120\n",
            "[4  3000] loss: 1.069\n",
            "[5  1000] loss: 0.999\n",
            "[5  2000] loss: 0.989\n",
            "[5  3000] loss: 0.954\n",
            "[6  1000] loss: 0.904\n",
            "[6  2000] loss: 0.887\n",
            "[6  3000] loss: 0.862\n",
            "[7  1000] loss: 0.814\n",
            "[7  2000] loss: 0.813\n",
            "[7  3000] loss: 0.796\n",
            "[8  1000] loss: 0.763\n",
            "[8  2000] loss: 0.745\n",
            "[8  3000] loss: 0.769\n",
            "[9  1000] loss: 0.705\n",
            "[9  2000] loss: 0.710\n",
            "[9  3000] loss: 0.731\n",
            "[10  1000] loss: 0.675\n",
            "[10  2000] loss: 0.674\n",
            "[10  3000] loss: 0.674\n",
            "[11  1000] loss: 0.630\n",
            "[11  2000] loss: 0.637\n",
            "[11  3000] loss: 0.642\n",
            "[12  1000] loss: 0.587\n",
            "[12  2000] loss: 0.605\n",
            "[12  3000] loss: 0.620\n",
            "[13  1000] loss: 0.557\n",
            "[13  2000] loss: 0.578\n",
            "[13  3000] loss: 0.596\n",
            "[14  1000] loss: 0.541\n",
            "[14  2000] loss: 0.559\n",
            "[14  3000] loss: 0.558\n",
            "[15  1000] loss: 0.523\n",
            "[15  2000] loss: 0.544\n",
            "[15  3000] loss: 0.530\n",
            "[16  1000] loss: 0.492\n",
            "[16  2000] loss: 0.501\n",
            "[16  3000] loss: 0.533\n",
            "[17  1000] loss: 0.479\n",
            "[17  2000] loss: 0.497\n",
            "[17  3000] loss: 0.502\n",
            "[18  1000] loss: 0.445\n",
            "[18  2000] loss: 0.479\n",
            "[18  3000] loss: 0.489\n",
            "[19  1000] loss: 0.443\n",
            "[19  2000] loss: 0.454\n",
            "[19  3000] loss: 0.462\n",
            "[20  1000] loss: 0.405\n",
            "[20  2000] loss: 0.432\n",
            "[20  3000] loss: 0.459\n",
            "[21  1000] loss: 0.408\n",
            "[21  2000] loss: 0.427\n",
            "[21  3000] loss: 0.423\n",
            "[22  1000] loss: 0.390\n",
            "[22  2000] loss: 0.393\n",
            "[22  3000] loss: 0.415\n",
            "[23  1000] loss: 0.372\n",
            "[23  2000] loss: 0.392\n",
            "[23  3000] loss: 0.403\n",
            "[24  1000] loss: 0.354\n",
            "[24  2000] loss: 0.382\n",
            "[24  3000] loss: 0.392\n",
            "[25  1000] loss: 0.343\n",
            "[25  2000] loss: 0.366\n",
            "[25  3000] loss: 0.370\n",
            "[26  1000] loss: 0.330\n",
            "[26  2000] loss: 0.353\n",
            "[26  3000] loss: 0.368\n",
            "[27  1000] loss: 0.330\n",
            "[27  2000] loss: 0.341\n",
            "[27  3000] loss: 0.359\n",
            "[28  1000] loss: 0.311\n",
            "[28  2000] loss: 0.315\n",
            "[28  3000] loss: 0.349\n",
            "[29  1000] loss: 0.298\n",
            "[29  2000] loss: 0.329\n",
            "[29  3000] loss: 0.333\n",
            "[30  1000] loss: 0.290\n",
            "[30  2000] loss: 0.310\n",
            "[30  3000] loss: 0.333\n",
            "[31  1000] loss: 0.282\n",
            "[31  2000] loss: 0.296\n",
            "[31  3000] loss: 0.312\n",
            "[32  1000] loss: 0.267\n",
            "[32  2000] loss: 0.302\n",
            "[32  3000] loss: 0.313\n",
            "[33  1000] loss: 0.261\n",
            "[33  2000] loss: 0.298\n",
            "[33  3000] loss: 0.296\n",
            "[34  1000] loss: 0.258\n",
            "[34  2000] loss: 0.292\n",
            "[34  3000] loss: 0.289\n",
            "[35  1000] loss: 0.256\n",
            "[35  2000] loss: 0.269\n",
            "[35  3000] loss: 0.281\n",
            "[36  1000] loss: 0.246\n",
            "[36  2000] loss: 0.263\n",
            "[36  3000] loss: 0.272\n",
            "[37  1000] loss: 0.228\n",
            "[37  2000] loss: 0.256\n",
            "[37  3000] loss: 0.273\n",
            "[38  1000] loss: 0.226\n",
            "[38  2000] loss: 0.250\n",
            "[38  3000] loss: 0.252\n",
            "[39  1000] loss: 0.223\n",
            "[39  2000] loss: 0.239\n",
            "[39  3000] loss: 0.249\n",
            "[40  1000] loss: 0.213\n",
            "[40  2000] loss: 0.238\n",
            "[40  3000] loss: 0.243\n",
            "[41  1000] loss: 0.209\n",
            "[41  2000] loss: 0.223\n",
            "[41  3000] loss: 0.246\n",
            "[42  1000] loss: 0.206\n",
            "[42  2000] loss: 0.219\n",
            "[42  3000] loss: 0.241\n",
            "[43  1000] loss: 0.191\n",
            "[43  2000] loss: 0.218\n",
            "[43  3000] loss: 0.222\n",
            "[44  1000] loss: 0.202\n",
            "[44  2000] loss: 0.205\n",
            "[44  3000] loss: 0.216\n",
            "[45  1000] loss: 0.185\n",
            "[45  2000] loss: 0.213\n",
            "[45  3000] loss: 0.224\n",
            "[46  1000] loss: 0.178\n",
            "[46  2000] loss: 0.198\n",
            "[46  3000] loss: 0.212\n",
            "[47  1000] loss: 0.174\n",
            "[47  2000] loss: 0.203\n",
            "[47  3000] loss: 0.208\n",
            "[48  1000] loss: 0.175\n",
            "[48  2000] loss: 0.200\n",
            "[48  3000] loss: 0.205\n",
            "[49  1000] loss: 0.173\n",
            "[49  2000] loss: 0.193\n",
            "[49  3000] loss: 0.193\n",
            "[50  1000] loss: 0.168\n",
            "[50  2000] loss: 0.182\n",
            "[50  3000] loss: 0.191\n",
            "[51  1000] loss: 0.176\n",
            "[51  2000] loss: 0.177\n",
            "[51  3000] loss: 0.194\n",
            "[52  1000] loss: 0.164\n",
            "[52  2000] loss: 0.172\n",
            "[52  3000] loss: 0.181\n",
            "[53  1000] loss: 0.165\n",
            "[53  2000] loss: 0.180\n",
            "[53  3000] loss: 0.189\n",
            "[54  1000] loss: 0.152\n",
            "[54  2000] loss: 0.170\n",
            "[54  3000] loss: 0.171\n",
            "[55  1000] loss: 0.161\n",
            "[55  2000] loss: 0.163\n",
            "[55  3000] loss: 0.176\n",
            "[56  1000] loss: 0.154\n",
            "[56  2000] loss: 0.162\n",
            "[56  3000] loss: 0.178\n",
            "[57  1000] loss: 0.152\n",
            "[57  2000] loss: 0.160\n",
            "[57  3000] loss: 0.165\n",
            "[58  1000] loss: 0.148\n",
            "[58  2000] loss: 0.165\n",
            "[58  3000] loss: 0.161\n",
            "[59  1000] loss: 0.137\n",
            "[59  2000] loss: 0.153\n",
            "[59  3000] loss: 0.161\n",
            "[60  1000] loss: 0.137\n",
            "[60  2000] loss: 0.151\n",
            "[60  3000] loss: 0.158\n",
            "[61  1000] loss: 0.141\n",
            "[61  2000] loss: 0.148\n",
            "[61  3000] loss: 0.151\n",
            "[62  1000] loss: 0.133\n",
            "[62  2000] loss: 0.152\n",
            "[62  3000] loss: 0.153\n",
            "[63  1000] loss: 0.139\n",
            "[63  2000] loss: 0.138\n",
            "[63  3000] loss: 0.147\n",
            "[64  1000] loss: 0.124\n",
            "[64  2000] loss: 0.142\n",
            "[64  3000] loss: 0.155\n",
            "[65  1000] loss: 0.119\n",
            "[65  2000] loss: 0.138\n",
            "[65  3000] loss: 0.154\n",
            "[66  1000] loss: 0.121\n",
            "[66  2000] loss: 0.137\n",
            "[66  3000] loss: 0.145\n",
            "[67  1000] loss: 0.122\n",
            "[67  2000] loss: 0.137\n",
            "[67  3000] loss: 0.133\n",
            "[68  1000] loss: 0.122\n",
            "[68  2000] loss: 0.131\n",
            "[68  3000] loss: 0.139\n",
            "[69  1000] loss: 0.118\n",
            "[69  2000] loss: 0.147\n",
            "[69  3000] loss: 0.126\n",
            "[70  1000] loss: 0.112\n",
            "[70  2000] loss: 0.126\n",
            "[70  3000] loss: 0.134\n",
            "[71  1000] loss: 0.115\n",
            "[71  2000] loss: 0.129\n",
            "[71  3000] loss: 0.131\n",
            "[72  1000] loss: 0.116\n",
            "[72  2000] loss: 0.126\n",
            "[72  3000] loss: 0.122\n",
            "[73  1000] loss: 0.112\n",
            "[73  2000] loss: 0.118\n",
            "[73  3000] loss: 0.125\n",
            "[74  1000] loss: 0.115\n",
            "[74  2000] loss: 0.123\n",
            "[74  3000] loss: 0.124\n",
            "[75  1000] loss: 0.112\n",
            "[75  2000] loss: 0.111\n",
            "[75  3000] loss: 0.130\n",
            "[76  1000] loss: 0.105\n",
            "[76  2000] loss: 0.118\n",
            "[76  3000] loss: 0.123\n",
            "[77  1000] loss: 0.100\n",
            "[77  2000] loss: 0.119\n",
            "[77  3000] loss: 0.123\n",
            "[78  1000] loss: 0.104\n",
            "[78  2000] loss: 0.110\n",
            "[78  3000] loss: 0.123\n",
            "[79  1000] loss: 0.099\n",
            "[79  2000] loss: 0.115\n",
            "[79  3000] loss: 0.114\n",
            "[80  1000] loss: 0.099\n",
            "[80  2000] loss: 0.118\n",
            "[80  3000] loss: 0.107\n",
            "[81  1000] loss: 0.112\n",
            "[81  2000] loss: 0.113\n",
            "[81  3000] loss: 0.105\n",
            "[82  1000] loss: 0.100\n",
            "[82  2000] loss: 0.109\n",
            "[82  3000] loss: 0.113\n",
            "[83  1000] loss: 0.107\n",
            "[83  2000] loss: 0.109\n",
            "[83  3000] loss: 0.109\n",
            "[84  1000] loss: 0.094\n",
            "[84  2000] loss: 0.103\n",
            "[84  3000] loss: 0.116\n",
            "[85  1000] loss: 0.098\n",
            "[85  2000] loss: 0.110\n",
            "[85  3000] loss: 0.105\n",
            "[86  1000] loss: 0.097\n",
            "[86  2000] loss: 0.105\n",
            "[86  3000] loss: 0.115\n",
            "[87  1000] loss: 0.091\n",
            "[87  2000] loss: 0.099\n",
            "[87  3000] loss: 0.113\n",
            "[88  1000] loss: 0.085\n",
            "[88  2000] loss: 0.098\n",
            "[88  3000] loss: 0.104\n",
            "[89  1000] loss: 0.092\n",
            "[89  2000] loss: 0.098\n",
            "[89  3000] loss: 0.106\n",
            "[90  1000] loss: 0.093\n",
            "[90  2000] loss: 0.103\n",
            "[90  3000] loss: 0.101\n",
            "[91  1000] loss: 0.089\n",
            "[91  2000] loss: 0.094\n",
            "[91  3000] loss: 0.098\n",
            "[92  1000] loss: 0.089\n",
            "[92  2000] loss: 0.095\n",
            "[92  3000] loss: 0.094\n",
            "[93  1000] loss: 0.086\n",
            "[93  2000] loss: 0.096\n",
            "[93  3000] loss: 0.107\n",
            "[94  1000] loss: 0.083\n",
            "[94  2000] loss: 0.101\n",
            "[94  3000] loss: 0.100\n",
            "[95  1000] loss: 0.087\n",
            "[95  2000] loss: 0.097\n",
            "[95  3000] loss: 0.094\n",
            "[96  1000] loss: 0.089\n",
            "[96  2000] loss: 0.081\n",
            "[96  3000] loss: 0.100\n",
            "[97  1000] loss: 0.085\n",
            "[97  2000] loss: 0.091\n",
            "[97  3000] loss: 0.091\n",
            "[98  1000] loss: 0.079\n",
            "[98  2000] loss: 0.089\n",
            "[98  3000] loss: 0.093\n",
            "[99  1000] loss: 0.082\n",
            "[99  2000] loss: 0.090\n",
            "[99  3000] loss: 0.092\n",
            "[100  1000] loss: 0.079\n",
            "[100  2000] loss: 0.090\n",
            "[100  3000] loss: 0.085\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNEiUQrPgQ-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef6b6d4d-d646-4108-85e1-f6b644eaf47b"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 79 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}